{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T23:12:40.954932Z",
     "start_time": "2024-10-08T23:12:40.866453Z"
    }
   },
   "id": "33768d0076b0f95d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-09T21:42:56.796983Z",
     "start_time": "2024-10-09T21:42:55.026369Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "class DataPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.model_to_make_mapping = None\n",
    "        self.overall_medians = {}\n",
    "\n",
    "    def fit(self, train_df):\n",
    "        \"\"\"\n",
    "        Fit the preprocessing on the training data to create mappings and overall statistics.\n",
    "        \"\"\"\n",
    "        # Create model_to_make mapping using non-null entries in the training dataset\n",
    "        self.model_to_make_mapping = train_df.dropna(subset=['make']).set_index('model')['make'].str.lower().to_dict()\n",
    "\n",
    "        # Calculate overall medians for columns where necessary\n",
    "        self.overall_medians = {\n",
    "            'manufactured': train_df['manufactured'].median(),\n",
    "            'power': train_df['power'].median(),\n",
    "            'mileage': train_df['mileage'].median(),\n",
    "            'engine_cap': train_df['engine_cap'].median(),\n",
    "            'depreciation': train_df['depreciation'].median(),\n",
    "            'road_tax': train_df['road_tax'].mean(),\n",
    "            'dereg_value': train_df['dereg_value'].mean(),\n",
    "            'coe': train_df['coe'].mean(),\n",
    "            'omv': train_df['omv'].mean(),\n",
    "            'arf': train_df['arf'].mean(),\n",
    "        }\n",
    "\n",
    "    def fill_missing_make(self, df):\n",
    "        \"\"\"\n",
    "        Fill missing values in the 'make' column using the 'model' column based on the mapping dictionary.\n",
    "        \"\"\"\n",
    "        def derive_make_from_model(row):\n",
    "            if pd.isnull(row['make']):\n",
    "                return self.model_to_make_mapping.get(row['model'].lower(), None)\n",
    "            else:\n",
    "                return row['make']\n",
    "\n",
    "        df['make'] = df.apply(derive_make_from_model, axis=1)\n",
    "\n",
    "    def fill_missing_with_group_mode_or_median(self, df, column_name):\n",
    "        \"\"\"\n",
    "        Fill missing values in a specified column using the mode of each group (model).\n",
    "        If the mode is not available, use the median of the training dataset.\n",
    "        \"\"\"\n",
    "        overall_median = self.overall_medians[column_name]\n",
    "        df[column_name] = df.groupby('model')[column_name].transform(\n",
    "            lambda x: x.fillna(x.mode().iloc[0] if not x.mode().empty else overall_median)\n",
    "        )\n",
    "\n",
    "    def fill_missing_with_group_mean_or_median(self, df, column_name):\n",
    "        \"\"\"\n",
    "        Fill missing values in a specified column using the mean of each group (model).\n",
    "        If the group mean is not available, use the overall mean from the training dataset.\n",
    "        \"\"\"\n",
    "        overall_mean = self.overall_medians[column_name]\n",
    "        df[column_name] = df.groupby('model')[column_name].transform(\n",
    "            lambda x: x.fillna(x.mean() if not x.mode().empty else overall_mean)\n",
    "        )\n",
    "\n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Apply the preprocessing steps to a dataset (training or test) using the fitted parameters.\n",
    "        \"\"\"\n",
    "        # Fill missing 'make' values using model information\n",
    "        self.fill_missing_make(df)\n",
    "\n",
    "        # Fill missing values for other columns using appropriate methods\n",
    "        self.fill_missing_with_group_mode_or_median(df, 'manufactured')\n",
    "        self.fill_missing_with_group_mode_or_median(df, 'power')\n",
    "        self.fill_missing_with_group_mode_or_median(df, 'mileage')\n",
    "        self.fill_missing_with_group_mode_or_median(df, 'engine_cap')\n",
    "        self.fill_missing_with_group_mode_or_median(df, 'depreciation')\n",
    "        self.fill_missing_with_group_mode_or_median(df, 'road_tax')\n",
    "        self.fill_missing_with_group_mean_or_median(df, 'dereg_value')\n",
    "        self.fill_missing_with_group_mean_or_median(df, 'coe')\n",
    "        self.fill_missing_with_group_mean_or_median(df, 'omv')\n",
    "        self.fill_missing_with_group_mean_or_median(df, 'arf')\n",
    "\n",
    "        return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T21:42:58.120732Z",
     "start_time": "2024-10-09T21:42:58.113180Z"
    }
   },
   "id": "907431efd08dbee",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T21:43:02.739794Z",
     "start_time": "2024-10-09T21:42:59.235255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "features = ['make', 'model','type_of_vehicle','category', 'manufactured', 'mileage', 'power','engine_cap', 'depreciation','road_tax','dereg_value','coe', 'omv', 'arf' ]\n",
    "target = 'price'\n",
    "numerical_features = ['manufactured', 'mileage', 'power','engine_cap', 'depreciation','road_tax','dereg_value','coe', 'omv', 'arf']\n",
    "categorical_features = ['make', 'model','type_of_vehicle','category']\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "train_df = train_df.dropna(subset=[target])\n",
    "\n",
    "preprocessor.fit(train_df)\n",
    "\n",
    "train_df = preprocessor.transform(train_df)\n",
    "test_df = preprocessor.transform(test_df)\n",
    "\n",
    "X = train_df[features]\n",
    "y = train_df[target]\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "4e965ec1ce855863",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T21:43:02.764100Z",
     "start_time": "2024-10-09T21:43:02.750800Z"
    }
   },
   "cell_type": "code",
   "source": "X_train.head()",
   "id": "b7da6d208d7e4a45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         make   model type_of_vehicle                  category  manufactured  \\\n",
       "23311   isuzu   npr85           truck            premium ad car        2019.0   \n",
       "23623   honda     fit       hatchback                   coe car        2009.0   \n",
       "1020   toyota  sienta             mpv  parf car, premium ad car        2018.0   \n",
       "12645   volvo     v40       hatchback                  parf car        2018.0   \n",
       "1533      kia  carens             mpv                  parf car        2015.0   \n",
       "\n",
       "        mileage  power  engine_cap  depreciation     road_tax  dereg_value  \\\n",
       "23311   14329.0  111.0      2999.0       14860.0  1462.160899      13348.0   \n",
       "23623   55000.0   73.0      1339.0       13510.0   885.000000        123.0   \n",
       "1020    80346.0   79.0      1496.0       14530.0   682.000000      25880.0   \n",
       "12645   68000.0  140.0      1969.0       15770.0  1176.000000      35358.0   \n",
       "1533   130000.0  122.0      1999.0       15540.0  1212.000000      20117.0   \n",
       "\n",
       "         coe      omv      arf  \n",
       "23311  22085  37994.0   1900.0  \n",
       "23623  14920  14211.0  14211.0  \n",
       "1020   38001  17199.0  17199.0  \n",
       "12645  36901  22799.0  23919.0  \n",
       "1533   58190  21074.0  21504.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>model</th>\n",
       "      <th>type_of_vehicle</th>\n",
       "      <th>category</th>\n",
       "      <th>manufactured</th>\n",
       "      <th>mileage</th>\n",
       "      <th>power</th>\n",
       "      <th>engine_cap</th>\n",
       "      <th>depreciation</th>\n",
       "      <th>road_tax</th>\n",
       "      <th>dereg_value</th>\n",
       "      <th>coe</th>\n",
       "      <th>omv</th>\n",
       "      <th>arf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23311</th>\n",
       "      <td>isuzu</td>\n",
       "      <td>npr85</td>\n",
       "      <td>truck</td>\n",
       "      <td>premium ad car</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>14329.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>2999.0</td>\n",
       "      <td>14860.0</td>\n",
       "      <td>1462.160899</td>\n",
       "      <td>13348.0</td>\n",
       "      <td>22085</td>\n",
       "      <td>37994.0</td>\n",
       "      <td>1900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23623</th>\n",
       "      <td>honda</td>\n",
       "      <td>fit</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>coe car</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>1339.0</td>\n",
       "      <td>13510.0</td>\n",
       "      <td>885.000000</td>\n",
       "      <td>123.0</td>\n",
       "      <td>14920</td>\n",
       "      <td>14211.0</td>\n",
       "      <td>14211.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>toyota</td>\n",
       "      <td>sienta</td>\n",
       "      <td>mpv</td>\n",
       "      <td>parf car, premium ad car</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>80346.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>14530.0</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>25880.0</td>\n",
       "      <td>38001</td>\n",
       "      <td>17199.0</td>\n",
       "      <td>17199.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12645</th>\n",
       "      <td>volvo</td>\n",
       "      <td>v40</td>\n",
       "      <td>hatchback</td>\n",
       "      <td>parf car</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>68000.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>15770.0</td>\n",
       "      <td>1176.000000</td>\n",
       "      <td>35358.0</td>\n",
       "      <td>36901</td>\n",
       "      <td>22799.0</td>\n",
       "      <td>23919.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1533</th>\n",
       "      <td>kia</td>\n",
       "      <td>carens</td>\n",
       "      <td>mpv</td>\n",
       "      <td>parf car</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>130000.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>15540.0</td>\n",
       "      <td>1212.000000</td>\n",
       "      <td>20117.0</td>\n",
       "      <td>58190</td>\n",
       "      <td>21074.0</td>\n",
       "      <td>21504.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T21:43:02.807708Z",
     "start_time": "2024-10-09T21:43:02.805809Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "33b80c52d34cbbe8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-09T21:43:02.833770Z",
     "start_time": "2024-10-09T21:43:02.830369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prepare a function to log model parameters and evaluation metrics. It should be in a single line saved to a csv for easy tracking.\n",
    "import csv\n",
    "import os\n",
    "\n",
    "def log_model(model_name, model_params, evaluation_metrics):\n",
    "    \"\"\"\n",
    "    Log the model parameters and evaluation metrics to a CSV file.\n",
    "    \"\"\"\n",
    "    # Create a new CSV file if it does not exist\n",
    "    if not os.path.exists('model_logs.csv'):\n",
    "        with open('model_logs.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['Model', 'Parameters', 'MAE', 'MSE', 'RMSE', 'R2'])\n",
    "\n",
    "    # Append the results to the CSV file\n",
    "    with open('model_logs.csv', 'a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([model_name, model_params, *evaluation_metrics])"
   ],
   "id": "593818b18266a2b8",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-09T21:43:02.873639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Define a PyTorch neural network model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class PyTorchRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, activation='ReLU', dropout_rate=0.2):\n",
    "        super(PyTorchRegressor, self).__init__()\n",
    "        self.activation = getattr(nn, activation)()  # Dynamically choose activation function\n",
    "\n",
    "        # Define the layers of the neural network\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim * 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.bn3 = nn.BatchNorm1d(hidden_dim * 2)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc4 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.bn4 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.fc5 = nn.Linear(hidden_dim, 1)  # Final output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.activation(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout2(self.activation(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout3(self.activation(self.bn3(self.fc3(x))))\n",
    "        x = self.dropout4(self.activation(self.bn4(self.fc4(x))))\n",
    "        x = self.fc5(x)  # Output layer for regression\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create a wrapper to integrate the PyTorch model with scikit-learn\n",
    "class PyTorchRegressorWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, hidden_dim=128, activation='ReLU', dropout_rate=0.2, learning_rate=0.001, epochs=100, batch_size=32):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None\n",
    "        self.input_dim = None  # To be determined during fitting\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Ensure that input data is converted to dense format and then to a NumPy array\n",
    "        X_dense = X if isinstance(X, np.ndarray) else X.toarray()\n",
    "        y_array = y.values if hasattr(y, 'values') else y\n",
    "\n",
    "        # Set input dimension based on the shape of the preprocessed data\n",
    "        self.input_dim = X_dense.shape[1]\n",
    "\n",
    "        # Initialize the more complex PyTorch model with the determined input dimension\n",
    "        self.model = PyTorchRegressor(self.input_dim, self.hidden_dim, self.activation, self.dropout_rate)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "\n",
    "        # Convert the data to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X_dense.astype(np.float32))\n",
    "        y_tensor = torch.tensor(y_array.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "        # Training loop with batch processing\n",
    "        dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "        dataloader = torch.utils.data.DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in dataloader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # Print the average loss for each epoch for monitoring purposes\n",
    "            if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "                print(f'Epoch [{epoch + 1}/{self.epochs}], Loss: {epoch_loss / len(dataloader):.4f}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Ensure that input data is converted to dense format and then to a NumPy array\n",
    "        X_dense = X if isinstance(X, np.ndarray) else X.toarray()\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.tensor(X_dense.astype(np.float32))\n",
    "            predictions = self.model(X_tensor).numpy()\n",
    "        return predictions.flatten()\n",
    "\n",
    "\n",
    "# Assume you have X_train and X_valid as your training and validation datasets\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# Preprocessing pipeline for categorical features (OneHotEncoder in this example)\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))  # Ensure dense output\n",
    "])\n",
    "\n",
    "# Preprocessing pipeline for numerical features\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the complete pipeline including preprocessing and the PyTorch model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', PyTorchRegressorWrapper())\n",
    "])\n",
    "\n",
    "# Define the parameter grid to search over\n",
    "param_grid = {\n",
    "    'model__hidden_dim': [64, 128, 256, 512],\n",
    "    'model__activation': ['ReLU', 'LeakyReLU', 'Tanh'],\n",
    "    'model__dropout_rate': [0.1, 0.2, 0.3, 0.5],\n",
    "    'model__learning_rate': [0.0001, 0.001, 0.01, 0.1],\n",
    "    'model__epochs': [100, 200, 300],\n",
    "    'model__batch_size': [32, 64, 128]\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=10,  # Number of different combinations to try\n",
    "    scoring='neg_mean_absolute_error',  # Use MAE as the scoring metric\n",
    "    cv=3,  # 3-fold cross-validation\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit the RandomizedSearchCV on the training data\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Save the best model to a file\n",
    "joblib.dump(best_model, 'best_pytorch_model.pkl')\n",
    "\n",
    "# Evaluate the model with the best parameters on validation data\n",
    "y_pred = best_model.predict(X_valid)\n",
    "mae = mean_absolute_error(y_valid, y_pred)\n",
    "mse = mean_squared_error(y_valid, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f'Best Parameters: {best_params}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n"
   ],
   "id": "f17850a89c295da4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-08T23:05:58.590533Z",
     "start_time": "2024-10-08T23:05:54.479243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming ComplexPyTorchRegressorWrapper and DataPreprocessor are already defined\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Load training and test data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Specify the target variable and features\n",
    "target = 'price'  # Replace with the actual target column name if different\n",
    "features = [col for col in train_df.columns if col != target]\n",
    "\n",
    "# Drop rows with missing target values in the training dataset\n",
    "train_df = train_df.dropna(subset=[target])\n",
    "\n",
    "# Fit the preprocessor on the training data\n",
    "preprocessor.fit(train_df)\n",
    "\n",
    "# Transform the training and test data using the fitted preprocessor\n",
    "train_df = preprocessor.transform(train_df)\n",
    "test_df = preprocessor.transform(test_df)\n",
    "test_df = test_df[features]  # Keep only the features in the test set\n",
    "\n",
    "# Load the best neural network model with the preprocessing pipeline\n",
    "best_model = joblib.load('best_neural_network_model.pkl')\n",
    "\n",
    "# Ensure that the target column 'price' is not in the test dataset\n",
    "if target in test_df.columns:\n",
    "    test_df = test_df.drop(columns=[target])\n",
    "\n",
    "# Convert the test data to a PyTorch tensor for prediction\n",
    "X_test_tensor = torch.tensor(test_df.values.astype(np.float32))\n",
    "\n",
    "# Make predictions using the test dataset with the best neural network model\n",
    "best_model.model.eval()  # Set the model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    test_predictions = best_model.model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "# Create a DataFrame with the predictions\n",
    "output = pd.DataFrame({'Id': test_df.index, 'Predicted': test_predictions})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "output.to_csv('predictions_v2.0.csv', index=False)\n",
    "\n",
    "# Display the first few rows of the output DataFrame\n",
    "print(output.head())\n"
   ],
   "id": "d5b1613da314774",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id      Predicted\n",
      "0   0   20147.593750\n",
      "1   1   35544.964844\n",
      "2   2  146033.046875\n",
      "3   3   79854.781250\n",
      "4   4   26131.941406\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "82f1cd8d0f43d666"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
